{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus reader is called by PickedReviewFunction below\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "\n",
    "PKL_PATTERN = r'(?!\\.)[\\w\\s\\d\\-]+\\.pickle'\n",
    "\n",
    "class SqliteCorpusReader(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self._cur = sqlite3.connect(path).cursor()\n",
    "        import nltk\n",
    "\n",
    "    def scores(self):\n",
    "        \"\"\"\n",
    "                Returns the review score\n",
    "                \"\"\"\n",
    "        self._cur.execute(\"SELECT score FROM reviews\")\n",
    "        scores = self._cur.fetchall()\n",
    "        for score in scores:\n",
    "            yield score\n",
    "\n",
    "\n",
    "    def texts(self):\n",
    "        \"\"\"\n",
    "        Returns the full review texts\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT content FROM content\")\n",
    "        texts = self._cur.fetchall()\n",
    "        for text in texts:\n",
    "            yield text\n",
    "\n",
    "    def ids(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT reviewid FROM content\")\n",
    "        ids = self._cur.fetchall()\n",
    "        for idx in ids:\n",
    "            yield idx\n",
    "\n",
    "    def ids_and_texts(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM content\")\n",
    "        results = self._cur.fetchall()\n",
    "        for idx,text in results:\n",
    "            yield idx,text\n",
    "\n",
    "    def scores_albums_artists_texts(self):\n",
    "        \"\"\"\n",
    "        Returns a generator with each review represented as a\n",
    "        (score, album name, artist name, review text) tuple\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "              SELECT S.score, L.label, A.artist, R.content\n",
    "              FROM [reviews] S\n",
    "              JOIN labels L ON S.reviewid=L.reviewid\n",
    "              JOIN artists A on L.reviewid=A.reviewid\n",
    "              JOIN content R ON A.reviewid=R.reviewid\n",
    "              \"\"\"\n",
    "        self._cur.execute(sql)\n",
    "        results = self._cur.fetchall()\n",
    "        for score,album,band,text in results:\n",
    "            yield (score,album,band,text)\n",
    "\n",
    "    def albums(self):\n",
    "        \"\"\"\n",
    "        Returns the names of albums being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM labels\")\n",
    "        albums = self._cur.fetchall()\n",
    "        for idx,album in albums:\n",
    "            yield idx,album\n",
    "\n",
    "    def artists(self):\n",
    "        \"\"\"\n",
    "        Returns the name of the artist being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM artists\")\n",
    "        artists = self._cur.fetchall()\n",
    "        for idx,artist in artists:\n",
    "            yield idx,artist\n",
    "\n",
    "    def genres(self):\n",
    "        \"\"\"\n",
    "        Returns the music genre of each review\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM genres\")\n",
    "        genres = self._cur.fetchall()\n",
    "        for idx,genre in genres:\n",
    "            yield idx,genre\n",
    "\n",
    "    def years(self):\n",
    "        \"\"\"\n",
    "        Returns the publication year of each review\n",
    "        Note: There are many missing values\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM years\")\n",
    "        years = self._cur.fetchall()\n",
    "        for idx,year in years:\n",
    "            yield idx,year\n",
    "\n",
    "    def paras(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs.\n",
    "        \"\"\"\n",
    "        for text in self.texts():\n",
    "            for paragraph in text:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences.\n",
    "        \"\"\"\n",
    "        for para in self.paras():\n",
    "            for sentence in nltk.sent_tokenize(para):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of words.\n",
    "        \"\"\"\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield word\n",
    "\n",
    "    def tagged_tokens(self):\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield nltk.pos_tag(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied picked reviews reader from github\n",
    "class PickledReviewsReader(CorpusReader):\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader\n",
    "        \"\"\"\n",
    "        CorpusReader.__init__(self, root, fileids, **kwargs)\n",
    "\n",
    "    def texts_scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the SqliteCorpusReader, this uses a generator\n",
    "        to achieve memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def reviews(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield text\n",
    "\n",
    "    def scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Return the scores\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield score\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for review in self.reviews(fileids):\n",
    "            for paragraph in review:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids):\n",
    "            # iterate through each sentence\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None):\n",
    "        for sent in self.sents(fileids):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tagged(fileids):\n",
    "            yield token[0]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "# imported joblib directly as sklearn.externals joblib is being deprecated\n",
    "import joblib \n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "# function to return list from corpus reviews \n",
    "def documents(corpus):\n",
    "    return list(corpus.reviews())\n",
    "\n",
    "# function to return scores for continuous value\n",
    "def continuous(corpus):\n",
    "    return list(corpus.scores())\n",
    "\n",
    "# function to return scores for categorical value\n",
    "def make_categories(corpus):\n",
    "    \"\"\"\n",
    "    terrible : 0.0 < y <= 3.0\n",
    "    okay : 3.0 < y <= 5.0\n",
    "    great : 5.0 < y <= 7.0\n",
    "    amazing : 7.0 < y <= 10.01\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.digitize(continuous(corpus), [0.0, 3.0, 5.0, 7.0, 10.1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(path, model, continuous=True, saveto=None, cv=12):\n",
    "    '''\n",
    "    \n",
    "    trains model from corpus at specified path, constructing cross-validation scores using the cv parameter,\n",
    "    then fitting the model on the full data.  Returns the scores.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # load the corpus data and labels for classification\n",
    "    # PickledReviews Reader from reader library no longer exist so used script for PickledReviewsReader\n",
    "    # corpus = PickledReviewsReader(path)\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    \n",
    "\n",
    "    # set X (training data) to corpus\n",
    "    X = documents(corpus)\n",
    "    # what does if continuous do?\n",
    "    if continuous:\n",
    "        # run continuous function \n",
    "        y = continuous(corpus)\n",
    "        # set scoring to r2_score\n",
    "        scoring = 'r2_score'\n",
    "    else:\n",
    "        # run categorical function\n",
    "        y = make_categorical(corpus)\n",
    "        # set scoring to f1_score\n",
    "        scoring = 'f1_score'\n",
    "        \n",
    "    # compute cross-validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    \n",
    "    # write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "        \n",
    "    # fit the model on entire dataset\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # return scores\n",
    "    return scores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CorpusReader: expected a string or a PathPointer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4e545af9cb11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     ])\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# run train model using regressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mregression_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0f3dc71a49dc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(path, model, continuous, saveto, cv)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# PickledReviews Reader from reader library no longer exist so used script for PickledReviewsReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# corpus = PickledReviewsReader(path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPickledReviewsReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-0162afa8a34d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, fileids, **kwargs)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \"\"\"\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mCorpusReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtexts_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, fileids, encoding, tagset)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPathPointer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CorpusReader: expected a string or a PathPointer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m# If `fileids` is a regexp, then expand it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: CorpusReader: expected a string or a PathPointer"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # TextNormalizer from transformer and transformers attempted, but don't exit\n",
    "    # from transformers import TextNormalizer\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    # reader for PickledReviewsReader no longer seems to exist, so loaded PickledReviewsReader script \n",
    "    # from reader import PickledReviewsReader\n",
    "    # also tried importing pickle and keras as alternatives to above\n",
    "    import pickle\n",
    "    # import keras\n",
    "    \n",
    "    # import sklearn libraries\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    # path to categorized comments\n",
    "    cpath = pd.read_json('categorized-comments.jsonl', lines=True, encoding='utf8')\n",
    "    \n",
    "    \n",
    "    \n",
    "    regressor = Pipeline([\n",
    "        ('norm', Normalizer()),\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('ann', MLPRegressor(hidden_layer_sizes=[500, 150], verbose=True))\n",
    "    ])\n",
    "    # run train model using regressor\n",
    "    regression_scores = train_model(cpath, regressor, continuous=True)\n",
    "        \n",
    "   \n",
    "    classifier = Pipeline([\n",
    "        ('norm', Normalizer()),\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('ann', MLPClassifier(hidden_layer_sizes=[500, 150], verbose=True))\n",
    "    ])     \n",
    "    \n",
    "    # run train model using classifier   \n",
    "    classifier_scores = train_model(cpath, classifier, continuous=False)\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "# Instantiate the classification model \n",
    "model = LogisticRegression()\n",
    "\n",
    "# define classes for confusion matrix\n",
    "classes = ['Approved','Not Approved']\n",
    "\n",
    "# define confusion matrix with logistic regression model and classes\n",
    "cm = ConfusionMatrix(model, classes=classes, percent=False)\n",
    "\n",
    "#Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X, y)\n",
    "\n",
    "#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "#and then creates the confusion_matrix from scikit learn.\n",
    "cm.score(X, y)\n",
    "\n",
    "# change fontsize of the labels in the figure\n",
    "for label in cm.ax.texts:\n",
    "    label.set_size(20)\n",
    "\n",
    "#How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "# Using classification report\n",
    "#%matplotlib inline\n",
    "\n",
    "# set the size of the figure and the font size \n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassificationReport(model, classes=classes)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(features_train, target_train)  \n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(features_test, target_test)  \n",
    "\n",
    "# print results\n",
    "g = visualizer.poof()\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Neural Network Classifier with Keras\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras import utils\n",
    "\n",
    "\n",
    "N_FEATURES = 5000\n",
    "N_CLASSES = 4\n",
    "\n",
    "\n",
    "def build_network():\n",
    "    '''\n",
    "    create a function that returns a compiled neural network\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    nn= Sequential()\n",
    "    nn.add(Dense(500, activation='relu', input_shape=N_FEATURES, ))\n",
    "    nn.add(Dense(150, activation='relu'))\n",
    "    nn.add(Dense(N_CLASSES, activation='softmax'))\n",
    "    nn.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']    \n",
    "    )\n",
    "    return nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(path, model, saveto=None, cv=12):\n",
    "    '''\n",
    "    \n",
    "    trains model from corpus at specified path and fits on full data.\n",
    "    if a saveto dictionary is specified, writes keras and sklearn pipeline components to disk separately.\n",
    "    return the scores.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    X = documents(corpus)\n",
    "    y = to_categorical(corpus)\n",
    "    \n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    # print(scores)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    if saveto:\n",
    "        model.steps[-1][1].model.save(saveto['keras_model'])\n",
    "        model.steps(pop(-1))\n",
    "        joblib.dump(model, saveto['sklearn_pipe'])\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    # from transformer import TextNormalizer\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from keras import utils\n",
    "\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('norm', Normalizer()),\n",
    "        ('vect', TfidfVectorizer(max_features=N_FEATURES)),\n",
    "        ('nn', KerasClassifier(build_fn=build_network,\n",
    "                              epochs=200,\n",
    "                              batch_size=128))\n",
    "    ])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpath = 'categorized-comments.csv'\n",
    "mpath = {\n",
    "    'keras_model' : 'keras_nn_h5', \n",
    "    'sklearn_pipe' : 'pipeline.pkl'\n",
    "}\n",
    "scores = train_model(cpath, pipeline, saveto=mpath, cv=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "# Instantiate the classification model \n",
    "model = LogisticRegression()\n",
    "\n",
    "# define classes for confusion matrix\n",
    "classes = ['Approved','Not Approved']\n",
    "\n",
    "# define confusion matrix with logistic regression model and classes\n",
    "cm = ConfusionMatrix(model, classes=classes, percent=False)\n",
    "\n",
    "#Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(features_train, target_train)\n",
    "\n",
    "#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "#and then creates the confusion_matrix from scikit learn.\n",
    "cm.score(features_test, target_test)\n",
    "\n",
    "# change fontsize of the labels in the figure\n",
    "for label in cm.ax.texts:\n",
    "    label.set_size(20)\n",
    "\n",
    "#How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "# Using classification report\n",
    "#%matplotlib inline\n",
    "\n",
    "# set the size of the figure and the font size \n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassificationReport(model, classes=classes)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(features_train, target_train)  \n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(features_test, target_test)  \n",
    "\n",
    "# print results\n",
    "g = visualizer.poof()\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Classifying Images\n",
    "\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# set that the color channel value will be first\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "# reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "# reshape test image into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "# rescale pixel intensity\n",
    "features_train = data_train/255\n",
    "features_test = data_test/255\n",
    "\n",
    "# one-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "# start neural network\n",
    "network = Sequential()\n",
    "\n",
    "# add convolutuonal layer with 64 filters, a 5x5 window, and ReLu activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5,5),\n",
    "                   input_shape=(channels, width, height),\n",
    "                   activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "# add max pooling layer with 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# add layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "\n",
    "# add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "# add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "# compile neural network\n",
    "network.compile(loss='categorical_crossentropy',  # cross-entropy\n",
    "                optimizer='rmsprop',  # root mean square propogation\n",
    "                metrics=['accuracy'])  # accuracy performance metric\n",
    "\n",
    "# train neural network\n",
    "network.fit(features_train,  # features\n",
    "           target_train,  # target\n",
    "           epochs=2,  # number of epochs\n",
    "           verbose=0,  # don't print description after each epoch\n",
    "           batch_size=1000,  # # of observations per batch\n",
    "           validation_data=(features_test, target_test))  # data for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "# Instantiate the classification model \n",
    "model = LogisticRegression()\n",
    "\n",
    "# define classes for confusion matrix\n",
    "classes = ['Approved','Not Approved']\n",
    "\n",
    "# define confusion matrix with logistic regression model and classes\n",
    "cm = ConfusionMatrix(model, classes=classes, percent=False)\n",
    "\n",
    "#Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(features_train, target_train)\n",
    "\n",
    "#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "#and then creates the confusion_matrix from scikit learn.\n",
    "cm.score(features_test, target_test)\n",
    "\n",
    "# change fontsize of the labels in the figure\n",
    "for label in cm.ax.texts:\n",
    "    label.set_size(20)\n",
    "\n",
    "#How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "# Using classification report\n",
    "#%matplotlib inline\n",
    "\n",
    "# set the size of the figure and the font size \n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassificationReport(model, classes=classes)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(features_train, target_train)  \n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(features_test, target_test)  \n",
    "\n",
    "# print results\n",
    "g = visualizer.poof()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
